{
  "metadata": {
    "testName": "IPAS_02",
    "subject": "L22",
    "series_no": "11411",
    "sourceFile": "IPAS_02中級_L22大數據處理分析與應用_11411.xlsx",
    "count": 50
  },
  "questions": [
    {
      "id": "IPAS_02_L22_11411_1",
      "content": "若某數據點的 Z 分數 (Z-Score) = 2, 請問代表下列哪一種意涵?",
      "A": "代表該數據點之原始數值為 2",
      "B": "該數據點比平均值低 2 個標準差",
      "C": "代表數據為異常值",
      "D": "該數據點比平均值高 2 個標準差",
      "Ans": "D",
      "exp": "Z 分數（Z）計算方式為 Z=σX−μ​。若 Z=2，表示數據點 X 比平均值 μ 高出 2 個標準差 σ。"
    },
    {
      "id": "IPAS_02_L22_11411_2",
      "content": "使用 Python 的 pandas 套件處理各商品銷售數據 (變數為 df) 時, 若需計算「總銷售額」欄位的敘述性統計量 (如平均值、標準差等), 應使用下列哪一種語法?",
      "A": "df['總銷售額'].sum()",
      "B": "df['總銷售額'].describe()",
      "C": "df['總銷售額'].sort_values()",
      "D": "df['總銷售額'].stats()",
      "Ans": "B",
      "exp": "在 pandas 中，.describe() 方法專門用於計算數值型欄位的敘述性統計量，包含計數、平均值、標準差、最大最小值及四分位數等。"
    },
    {
      "id": "IPAS_02_L22_11411_3",
      "content": "附圖  為某資料之分佈圖, 此資料之偏態 (Skewness) 值較有可能為下列哪個選項?",
      "A": "Skewness < 0",
      "B": "Skewness > 0",
      "C": "Skewness = 0",
      "D": "無法計算 Skewness",
      "Ans": "A",
      "exp": "圖中分佈的長尾延伸向左側（負值方向），顯示為負偏態 (Left-Skewed/Negative Skewness)。負偏態的分佈其偏態值 (Skewness) 會小於 0。"
    },
    {
      "id": "IPAS_02_L22_11411_4",
      "content": "累積分佈函數 (Cumulative Distribution Function, CDF) 可用於描述隨機變數的機率分佈特性, 其數學定義為下列何者?",
      "A": "機率密度函數 (Probability Density Function, PDF) 的平均值",
      "B": "機率密度函數 (Probability Density Function, PDF) 的積分",
      "C": "機率密度函數 (Probability Density Function, PDF) 的離散總和",
      "D": "機率密度函數 (Probability Density Function, PDF) 的標準差",
      "Ans": "B",
      "exp": "對於連續隨機變數，累積分佈函數 F(x) 的數學定義是其機率密度函數 (PDF) f(t) 從負無窮大到 x 的積分 (F(x)=∫−∞x​f(t)dt)。"
    },
    {
      "id": "IPAS_02_L22_11411_5",
      "content": "在進行資料前處理時, 若使用 Label Encoding 將類別變數轉換為數字型態, 下列何者為最常見的潛在風險?",
      "A": "無法處理缺值",
      "B": "會引入類別之間的虛假順序關係",
      "C": "無法擴展至新資料",
      "D": "記憶體佔用過高",
      "Ans": "B",
      "exp": "標籤編碼 (Label Encoding) 會將無序的類別 (如顏色) 轉換為整數 1,2,3...，這會讓模型誤以為這些數字之間存在數值上的大小順序或等級關係，即引入虛假順序關係。"
    },
    {
      "id": "IPAS_02_L22_11411_6",
      "content": "在進行資料分析時, 會遇到類別型 (Categorical) 與數值型 (Numerical) 資料格式。關於這兩種資料格式的處理, 下列敘述何者不正確?",
      "A": "One-Hot 編碼適用於無序類別資料，在高基數下可能造成維度爆炸問題",
      "B": "標籤編碼若應用於無序資料，可能導致模型誤將編碼值解讀為具數值大小關係的特徵",
      "C": "標準化 (Standardization) 透過將資料平移與縮放，使其平均值為 0、標準差為 1, 可在多數距離型演算法中改善收斂速度, 並同時將數值範圍壓縮至 0 至 1 之間",
      "D": "對連續變數進行分箱 (Binning) 可提升模型可解釋性，但若分段方式未依據資料分佈特性設計，可能導致資訊損失或邊界偏誤",
      "Ans": "C",
      "exp": "標準化 (Standardization) (Z-score) 的結果是使資料的平均值為 0、標準差為 1，但數值範圍並不限於 0 到 1 之間。將數值範圍壓縮至 0 到 1 之間是 Min-Max 正規化 (Normalization) 的特點。"
    },
    {
      "id": "IPAS_02_L22_11411_7",
      "content": "在資料庫的 ACID 特性中, 下列何者為「原子性 (Atomicity)」的正確定義?",
      "A": "所有資料欄位必須為相同型別",
      "B": "每次交易需以批次方式執行",
      "C": "交易不可分割, 需完全成功或完全失敗",
      "D": "系統會自動同步交易資料至所有節點",
      "Ans": "C",
      "exp": "原子性 (Atomicity) 確保一筆交易是一個不可分割的單元，要求交易中的所有操作必須全部完成（提交），或在失敗時全部撤銷（回覆），不會留下部分完成的狀態。"
    },
    {
      "id": "IPAS_02_L22_11411_8",
      "content": "資料科學家為分析顧客行為, 利用現有欄位「銷售金額」與「瀏覽次數」, 計算出新變數「銷售金額/瀏覽次數」。此動作屬於下列哪一類特徵工程方法?",
      "A": "特徵選擇 (Feature Selection)",
      "B": "特徵衍生 (Feature Derivation)",
      "C": "特徵轉換 (Feature Transformation)",
      "D": "分箱處理 (Binning)",
      "Ans": "B",
      "exp": "特徵衍生 (Feature Derivation) 是指透過組合、運算或聚合一個或多個原始特徵，來創造出一個具有新業務意義的新特徵，例如計算轉換率（銷售金額/瀏覽次數）。"
    },
    {
      "id": "IPAS_02_L22_11411_9",
      "content": "在進行數值特徵的標準化 (Normalization) 時, 若資料中存在極端值 (Outliers), 下列哪一種方法最適合使用?",
      "A": "Min-Max 正規化 (Min-Max Scaling)",
      "B": "Z-score 標準化 (Z-score Normalization)",
      "C": "穩健縮放 (Robust Scaling)",
      "D": "標準分箱 (Standard Binning)",
      "Ans": "C",
      "exp": "穩健縮放 (Robust Scaling) 使用中位數 (Median) 和四分位距 (IQR) 進行縮放。由於這兩個統計量對極端值不敏感，因此 Robust Scaling 比 Min-Max 或 Z-score 更穩健，特別適合處理存在 Outliers 的數據集。"
    },
    {
      "id": "IPAS_02_L22_11411_10",
      "content": "下列哪一種情境最適合應用異常偵測 (Anomaly Detection) 技術?",
      "A": "根據歷史銷售資料預測特定商品在旺季期間是否會出現供貨短缺, 以提前調整庫存策略",
      "B": "透過信用風險模型預測顧客是否可能發生違約, 以輔助核貸決策",
      "C": "即時分析金融交易資料流, 偵測與平常交易行為明顯不同的可疑交易紀錄",
      "D": "監控線上服務平台的使用者登入次數, 預測次日的登入量變化趨勢",
      "Ans": "C",
      "exp": "異常偵測 (Anomaly Detection) 專門用於識別與絕大多數數據點顯著不同的觀測值。偵測與平常行為明顯不同的可疑交易紀錄（如金融詐欺）是其典型應用場景。"
    },
    {
      "id": "IPAS_02_L22_11411_11",
      "content": "若一家公司需即時監控大量物聯網裝置的異常行為, 下列哪一種組合最適 合此應用?",
      "A": "傳統關聯式資料庫 + 圖形視覺化",
      "B": "批次資料處理 + 雲端備份",
      "C": "大數據平台 + 即時資料分析技術",
      "D": "Word 文件 + 手動標註",
      "Ans": "C",
      "exp": "即時監控大量 IoT 數據需要處理高通量串流資料，必須仰賴能處理海量數據的大數據平台（如雲端數據湖/倉儲）和能即時處理數據流的即時資料分析技術（如 Stream Processing 框架）。"
    },
    {
      "id": "IPAS_02_L22_11411_12",
      "content": "在處理分類問題時, 若某一類樣本數明顯少於其他類別, 研究人員可能採 用隨機過採樣 (Random Oversampling) 以平衡資料比例, 此方法最常造 成下列哪一種問題?",
      "A": "增加過擬合風險",
      "B": "降低模型的收斂速度",
      "C": "減少資料總筆數數量",
      "D": "導致訓練資料欄位缺失",
      "Ans": "A",
      "exp": "隨機過採樣 (Random Oversampling) 是簡單地複製少數類樣本。重複的樣本會導致模型過度學習這些少數類樣本的特定特徵，從而增加過擬合 (Overfitting) 的風險。"
    },
    {
      "id": "IPAS_02_L22_11411_13",
      "content": "下列何者為同態加密 (Homomorphic Encryption) 技術的核心特性?",
      "A": "將資料轉換為匿名識別碼以隱藏身分",
      "B": "對資料進行標準化處理以提升模型精度",
      "C": "自動偵測與排除異常值",
      "D": "可直接在加密狀態下進行數據運算",
      "Ans": "D",
      "exp": "同態加密 (Homomorphic Encryption) 允許對加密數據執行運算，並且運算結果在解密後與對原始數據進行運算的結果一致。其核心特性就是可直接在加密狀態下進行數據運算，實現數據隱私保護下的運算。"
    },
    {
      "id": "IPAS_02_L22_11411_14",
      "content": "某組資料共 10 項標籤如下: A, A, A, A, A, B, B, B, B, B。若該標籤僅有 A、B 兩種, 請問這組資料的「正規化吉尼不純度 (Normalized Gini impurity)」為何?",
      "A": "0.5",
      "B": "0.42",
      "C": "0.84",
      "D": "1",
      "Ans": "D",
      "exp": "Gini Impurity G=1−(0.52+0.52)=0.5。對於二元分類，最大不純度為 0.5 (當兩類比例相等時)。正規化吉尼不純度通常定義為 max(G)G​=0.50.5​=1。"
    },
    {
      "id": "IPAS_02_L22_11411_15",
      "content": "某家客服中心統計資料發現, 平均每小時會接到約 20 通顧客來電, 但每分鐘的來電數量不固定, 可能為 0、1、2 通不等。這些來電事件彼此獨立, 且在短時間內, 發生的機率與時間長短成正比。若要以機率模型描述「每分鐘接到幾通來電」的機率分佈, 下列哪一種最適合使用?",
      "A": "均勻分佈 (Uniform distribution)",
      "B": "指數分佈 (Exponential distribution)",
      "C": "卜瓦松分佈 (Poisson distribution)",
      "D": "常態分佈 (Normal distribution)",
      "Ans": "C",
      "exp": "卜瓦松分佈適用於描述在固定時間/空間內，獨立事件發生次數的機率。題目中的「每分鐘接到幾通來電」完全符合此特性，且平均發生率固定。"
    },
    {
      "id": "IPAS_02_L22_11411_16",
      "content": "某金融科技公司以 Z 分數（Z-Score）監控交易金額異常狀況。若交易金額平均為新台幣 2,000 元，標準差為 400 元，某筆交易金額為 3,200元，且公司以|Z| ≥ 3 判定為異常值（Outlier），下列判斷何者最為正確？",
      "A": "該筆交易的 Z 分數為 3，應標記為異常值；",
      "B": "該筆交易的 Z 分數為 2.5，屬於合理變異範圍；",
      "C": "該筆交易的 Z 分數為 2，顯示模型標準差估計過高；",
      "D": "該筆交易的 Z 分數為 1.5，無須納入異常檢測",
      "Ans": "A",
      "exp": "步驟與計算這是一個典型的 Z 分數（Z-Score）計算問題。定義 Z 分數公式：$$Z = \\frac{X - \\mu}{\\sigma}$$其中：$X$ 是單筆交易金額（要計算的數據點）$\\mu$ 是交易金額平均值（母體或樣本平均數）$\\sigma$ 是交易金額標準差代入已知數值：交易金額 $X = 3,200$ 元平均值 $\\mu = 2,000$ 元標準差 $\\sigma = 400$ 元計算 Z 分數：$$Z = \\frac{3,200 - 2,000}{400} = \\frac{1,200}{400} = 3$$判定是否為異常值：公司判定的異常值標準為 $|Z| \\ge 3$。計算結果 $|3| = 3$，滿足判定條件。結論該筆交易的 Z 分數為 3。因為 $|Z|=3$ 滿足異常值判定標準 $|Z| \\ge 3$，所以應標記為異常值。因此，選項 (A) 該筆交易的 Z 分數為 3，應標記為異常值 是正確的。"
    },
    {
      "id": "IPAS_02_L22_11411_17",
      "content": "某電商公司欲利用顧客行為資料建立消費預測模型, 其中「會員等級」欄位包含「一般、白金、黑卡」三種類別。若模型採用梯度提升樹 (Gradient Boosting Tree) 演算法, 資料科學家在進行特徵編碼時應特別注意下列何種情況?",
      "A": "應優先採用獨熱編碼 (One-Hot Encoding), 以減少類別之間的相依性與記憶體使用量",
      "B": "直接使用標籤編碼 (Label Encoding) 可能使模型誤判類別間存在順序關係, 導致特徵重要性偏誤",
      "C": "使用目標編碼 (Target Encoding) 會自動消除過擬合 (Overfitting) 風險",
      "D": "若類別數量較少, 建議先使用主成分分析 (Principal Component Analysis, PCA) 進行降維",
      "Ans": "B",
      "exp": "即使是樹狀模型，標籤編碼仍會將類別轉換為數值，若類別之間沒有對應的數值順序（雖然會員等級一般有），模型可能將編碼值直接視為數值大小關係，從而影響決策樹的分裂標準和特徵重要性。"
    },
    {
      "id": "IPAS_02_L22_11411_18",
      "content": "某人工智慧團隊使用分散式資料庫 (Distributed Database) 儲存模型訓練資料, 並在更新訓練樣本時啟用多節點交易。若其中一個節點在交易過程中發生錯誤, 但系統仍確保整體資料不會出現部分更新、最終狀態維持一致, 下列何者最能說明此現象?",
      "A": "系統透過原子性 (Atomicity) 確保交易必須全部成功或全部回復 (Rollback)",
      "B": "系統透過一致性 (Consistency) 確保交易完成後資料符合完整性規則",
      "C": "系統透過隔離性 (Isolation) 避免多筆交易同時存取或修改相同資料",
      "D": "系統透過持久性 (Durability) 確保交易一旦提交, 其結果將永久保留於資料庫中",
      "Ans": "A",
      "exp": "該描述指交易的全有或全無 (All or Nothing) 特性，即使在分散式系統中，錯誤發生時仍會回復，這是 原子性 (Atomicity) 的核心體現。"
    },
    {
      "id": "IPAS_02_L22_11411_19",
      "content": "某製造企業導入上萬台物聯網 (IoT) 感測器以進行設備健康監測。系統需在毫秒級回應異常事件, 並同時將完整資料保留於雲端供後續 AI 模型訓練與分析。若企業希望兼顧即時性、資料完整性與可擴展性, 下列哪一種資料流程設計最符合此目標?",
      "A": "感測器 → 雲端 API Gateway → 分散式資料庫 → 批次特徵工程 → 模型推論",
      "B": "感測器 → MQTT Broker → 雲端資料倉儲 → 即時儀表板 → 模型再訓練",
      "C": "感測器 → 邊緣運算節點 → 流式資料處理框架 (Stream Processing Framework) → 雲端資料湖 → 模型推論",
      "D": "感測器 → 本地快取層 → RESTful API → 雲端報表系統 → 模型批次更新",
      "Ans": "C",
      "exp": "要滿足毫秒級即時性（流式處理）和資料完整性（資料湖），最優架構是：先透過邊緣運算進行快速處理，再利用流式資料處理框架處理高通量數據，最後將完整原始資料儲存於雲端資料湖。"
    },
    {
      "id": "IPAS_02_L22_11411_20",
      "content": "某銀行計畫將信用風險評估模型部署至雲端平台, 以便即時分析客戶交易行為。由於涉及大量敏感金融資料, 銀行要求雲端服務商在不解密原始資料的情況下仍能執行模型運算。為達成此目標, 最適合採用下列哪一項技術?",
      "A": "在上傳資料前進行匿名化 (Anonymization), 僅保留可識別代碼供比對使用",
      "B": "利用雜湊 (Hash) 函數轉換資料, 以確保模型可追蹤但無法還原個資",
      "C": "採用資料本地化 (Data Localization) 策略, 將所有模型訓練限制於內部伺服器中",
      "D": "透過同態加密 (Homomorphic Encryption), 讓雲端系統能直接在加密資料上執行運算, 解密後結果與原始資料一致",
      "Ans": "D",
      "exp": "同態加密技術的特點是允許在密文上直接進行運算，這完美解決了在雲端平台上處理敏感金融資料時，既要保護隱私，又要能執行 AI 模型運算的需求。"
    },
    {
      "id": "IPAS_02_L22_11411_21",
      "content": "某資料分析師設計在業務績效報告時, 希望單一頁面中同時呈現多區域不同產品線的銷售趨勢變化, 並確保主管能在短時間內掌握整體資料走向。若依據 Edward Rolf Tufte 的數據密度 (Data Density) 原則, 下列哪一種設計方式最能符合該概念?",
      "A": "將每個區域的銷售資料分成多張獨立折線圖, 以避免資訊重疊",
      "B": "使用顏色區分產品線, 於同一圖表中整合多區域趨勢線, 保持比例一致且標註清晰",
      "C": "移除所有輔助線與標籤, 僅保留主要折線以凸顯趨勢",
      "D": "將資料轉換為表格形式, 確保數值精確呈現並取代圖表視覺化",
      "Ans": "B",
      "exp": "數據密度原則強調在最小的空間內傳達最多的資訊。將多個區域與產品線的數據整合在同一圖表中（而非拆分），並使用顏色、清晰標註等輔助區分，能有效提升圖表的數據密度與資訊傳遞效率。"
    },
    {
      "id": "IPAS_02_L22_11411_22",
      "content": "某投資研究員希望分析四檔科技類股 (A,B,C,D) 每日報酬率的變化趨勢, 以判斷這些股票之間是否存在高度相關性與共變動性, 並評估投資組合分散風險的程度。若研究員希望以單一圖表快速呈現各股票間的關聯強度與方向, 下列哪一種視覺化呈現方式最適合?",
      "A": "為每檔股票各自繪製直方圖 (Histogram) 以比較報酬率分佈",
      "B": "針對任兩檔股票繪製散佈圖並加上趨勢線 (Regression Line)",
      "C": "使用雙軸折線圖 (Dual-axis Line Chart) 同時顯示四檔股價變化",
      "D": "熱力圖 (Heatmap) 配合相關係數矩陣 (Correlation Matrix)",
      "Ans": "D",
      "exp": "相關係數矩陣搭配熱力圖能以單一圖表直觀呈現多個變數之間兩兩配對的關聯強度與方向（正相關/負相關），是分析變數間相關性的標準方法。"
    },
    {
      "id": "IPAS_02_L22_11411_23",
      "content": "某研究團隊以單樣本 t 檢定 (one-sample t-test) 檢驗「新行銷策略後的平均月銷售額是否與原本的 100 萬元不同」, 顯著水準設定為 α=0.05。檢定結果顯示: p 值 =0.08, 且 95% 信賴區間為 [95 萬元, 108 萬元]。根據上述結果, 下列敘述何者正確?",
      "A": "因 p 值 <0.05, 可拒絕虛無假設",
      "B": "若顯著水準改為 0.10, 仍不顯著",
      "C": "因 100 萬元落在信賴區間內, 無法拒絕虛無假設",
      "D": "信賴區間寬度僅與顯著水準有關",
      "Ans": "C",
      "exp": "當虛無假設的值（100 萬元）落在 95% 信賴區間 [95,108] 內時，表示該值是合理的母體平均數值，故無法拒絕虛無假設。此外，p 值 0.08 大於 α=0.05，也同樣表示無法拒絕 H0​。"
    },
    {
      "id": "IPAS_02_L22_11411_24",
      "content": "某企業建置生成式 AI 系統, 利用大量客服紀錄與產品評論資料訓練語言模型, 在資料來源多樣, 且包含非結構化文字、影像與表格資訊, 團隊希望在不降低模型效能的前提下, 提升資料處理效率與一致性, 下列哪一種資料處理策略最適合?",
      "A": "建立資料湖 (Data Lake) 結構, 並以 Apache Spark 或 Ray 進行分散式資料預處理與特徵抽取, 再串接至模型訓練管線 (Pipeline)",
      "B": "採用單節點高效能伺服器搭配批次處理模式, 集中執行資料清理與格式轉換",
      "C": "將所有文字資料轉換為向量, 並以資料庫索引方式直接餵入語言模型訓練",
      "D": "使用生成式模型先行自動清理資料內容, 再將結果輸入至下游訓練流程",
      "Ans": "A",
      "exp": "處理多樣化、非結構化的大數據，最有效的策略是採用資料湖來儲存原始數據，並結合 Apache Spark/Ray 等分散式運算框架進行大規模的並行預處理，以確保效率與一致性。"
    },
    {
      "id": "IPAS_02_L22_11411_25",
      "content": "某電商資料團隊繪製顧客單筆消費金額的箱型圖後發現: 四分位距 (IQR) 範圍極小, 但上鬚線拉得很長, 且在高金額區域有多筆離群值。若希望協助行銷部門依據消費層級設計分群策略, 下列哪一種視覺化方式最有助於凸顯不同消費層級間的差異?",
      "A": "以對數刻度繪製箱型圖或長條圖, 放大高金額消費族群的變化差異",
      "B": "移除所有離群值, 確保資料呈現集中分布",
      "C": "採用等距分箱 (Equal-Width Binning) 方式分群",
      "D": "改以折線圖 (Line Chart) 觀察時間變化趨勢",
      "Ans": "A",
      "exp": "資料分佈高度右偏且存在極端值，使用對數刻度 (Logarithmic Scale) 繪圖是一種標準的處理方法。對數刻度可以壓縮高數值範圍，使極端值靠近，從而放大低/中數值區的差異，使高金額消費族群的細節變化得以清晰呈現。"
    },
    {
      "id": "IPAS_02_L22_11411_26",
      "content": "某串流影音平台運用關聯規則學習 (Association Rule Learning) 分析用戶的觀影行為, 發現若使用者觀看了科幻影集, 則有較高機率接著觀看超級英雄電影。分析顯示, 同時觀看這兩種類型的使用者約佔全部觀影紀錄的 12%, 而觀看科幻影集的使用者中, 有 50% 也觀看了超級英雄電影, 該規則的提升度 (Lift) 為 1.8。根據上述資訊, 下列哪一項推論最為正確?",
      "A": "支持度 (Support) 過低, 代表此規則不具任何商業價值",
      "B": "提升度 (Lift) 大於 1 表示兩種類型內容無關, 僅屬於隨機重疊",
      "C": "信賴度 (Confidence) 為 50%, 代表觀看科幻影集者有明顯傾向觀看超級英雄電影",
      "D": "同時觀看比例僅 12%, 代表兩種類型互相排斥",
      "Ans": "C",
      "exp": "信賴度 (Confidence) $P(B"
    },
    {
      "id": "IPAS_02_L22_11411_27",
      "content": "某金融科技公司分析每日上億筆交易資料, 以監控客戶轉帳金額分佈與異常波動。由於資料量極大, 為兼顧效率與準確度, 團隊決定採用「近似分位數 (Approximate Quantile)」方法進行資料摘要統計。下列何者最能正確反映該技術的核心目的?",
      "A": "確保每個分位值的結果完全精確, 即使計算時間較長",
      "B": "利用機器學習模型預測分位數位置, 以減少統計計算量",
      "C": "僅能對結構化資料進行批次處理, 無法應用於即時資料流",
      "D": "在可容忍誤差範圍內, 快速估算分位值以支援即時分析",
      "Ans": "D",
      "exp": "近似分位數技術是為了解決大數據下無法在合理時間內計算精確分位數的問題。它的核心目的就是在犧牲極小的準確度的前提下，快速估算分位值，以滿足即時監控和分析對效率的要求。"
    },
    {
      "id": "IPAS_02_L22_11411_28",
      "content": "若在高維度 (>500 維) 的資料上應用 DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 演算法, 卻發現所有資料點皆被判定為雜訊 (Noise), 下列何者為最有可能的原因?",
      "A": "高維下距離變化趨同, 導致 ϵ (Epsilon) 閾值選擇失效",
      "B": "使用錯誤的距離函數 (Distance Function)",
      "C": "MinPts 參數設得太小",
      "D": "資料過度標準化導致特徵消失",
      "Ans": "A",
      "exp": "在高維度空間，數據點之間的距離差異會趨於一致（維度災難）。這使得 DBSCAN 難以找到合適的鄰域半徑 ϵ 來定義密度，導致大多數點的鄰域內都無法達到最小點數 (MinPts)，從而所有點都被判定為雜訊。"
    },
    {
      "id": "IPAS_02_L22_11411_29",
      "content": "某團隊在開發風險評估模型時, 使用主成分分析 (Principal Component Analysis, PCA) 進行降維。輸入資料包含三個數值欄位: 「交易金額 (單位: 新台幣)」、「交易次數 (次/月)」與「年齡 (歲)」, 其數值量級分別約為 105、101 與 102。分析人員直接將原始數據帶入 PCA, 結果第一主成分 (PC1) 幾乎完全由「交易金額」主導。下列哪一項作法或判斷最合理?",
      "A": "這是正常現象, 金額本身變異較大, 應主導主要成分",
      "B": "若改用特徵選擇法, 可自動解決變數量級問題",
      "C": "可刪除「交易金額」欄位以平衡各主成分的影響",
      "D": "在進行 PCA 前應先進行標準化 (Standardization), 以避免因數值尺度差異造成特徵偏誤",
      "Ans": "D",
      "exp": "PCA 基於變異數，由於「交易金額」的數值量級遠大於其他特徵，其變異數最大，因此自然會主導第一主成分。正確的處理方法是先進行 標準化 (Standardization)，消除數值尺度的差異，使所有特徵對主成分的貢獻基於其內部變異性，而非外部量級。"
    },
    {
      "id": "IPAS_02_L22_11411_30",
      "content": "某行銷團隊想了解「廣告預算」與「銷售金額」之間的關聯程度。經繪製散佈圖後發現兩者呈現明顯線性趨勢, 且資料中無明顯離群值 (Outliers)。若希望衡量兩者之間線性關係的強度與方向, 下列哪一種方法最適合?",
      "A": "均方根誤差 (Root Mean Squared Error, RMSE)",
      "B": "共變異數 (Covariance)",
      "C": "皮爾森相關係數 (Pearson Correlation Coefficient)",
      "D": "平均絕對誤差 (Mean Absolute Error, MAE)",
      "Ans": "C",
      "exp": "皮爾森相關係數是專門用於衡量兩個連續變數之間線性關係的強度和方向的標準化指標。"
    },
    {
      "id": "IPAS_02_L22_11411_31",
      "content": "某電商團隊觀察到, 每位顧客對廣告推播的點擊行為可視為一次伯努利試驗 (Bernoulli Trial), 單次點擊成功機率為 p=0.4。當推播對象擴增至 5,000 位顧客時, 團隊想快速預估「成功點擊總數」的分佈情形, 以進行模型效能模擬與預測。若希望以常態分佈 (Normal Distribution) 近似原始分佈, 下列哪一項判斷最為合理?",
      "A": "因樣本數極大, 可直接以常態分佈近似二項分佈 (Binomial Distribution)",
      "B": "只有當 np 與 n(1−p) 皆大於 5 時, 才能以常態分佈作近似",
      "C": "常態近似只適用於 p=0.5 的情況",
      "D": "無論樣本數多大, 二項分佈都不能以常態分佈近似",
      "Ans": "B",
      "exp": "常態分佈近似二項分佈的經驗法則要求** np 和 n(1−p) 皆大於或等於 5**（或 10）。本題中 n=5000,p=0.4，則 np=2000 且 n(1−p)=3000，皆滿足條件。"
    },
    {
      "id": "IPAS_02_L22_11411_32",
      "content": "某電信公司導入生成式 AI 客服系統, 利用過去對話紀錄與用戶行為資料訓練語言模型, 在資料治理與合規審查過程中, 團隊發現模型可能會在回答中生成包含真實姓名、電話或交易資訊的內容。為確保系統符合個資法及生成式 AI 的安全與隱私要求, 下列哪一項作法最符合實務可行及法規原則?",
      "A": "在訓練資料前進行資料匿名化 (Anonymization) 或偽匿名化 (Pseudonymization) 處理, 並建立輸出內容稽核機制",
      "B": "改以強化學習 (Reinforcement Learning) 微調模型, 使模型學習避免產出真實資訊",
      "C": "採用同態加密 (Homomorphic Encryption) 以加密所有文字輸入, 確保模型無法辨識任何個資",
      "D": "僅設定模型回覆時不顯示用戶姓名, 即可視為隱私防護完成",
      "Ans": "A",
      "exp": "解決生成式 AI 記憶效應導致的個資洩露問題，最根本的策略是從訓練數據源頭進行匿名化或偽匿名化，並配合輸出內容稽核機制進行二次檢查，符合法規與實務上的隱私保護原則。"
    },
    {
      "id": "IPAS_02_L22_11411_33",
      "content": "某金融機構的量化分析師在建立資產風險評估模型時, 發現報酬率資料分佈明顯非對稱, 且出現多次極端損失事件, 使得傳統假設常態分佈的模型無法準確反映真實風險。若希望在不依賴常態分佈假設的前提下, 採取更能捕捉資料極端情況的建模策略, 下列哪一種方法最為合適?",
      "A": "採用線性迴歸模型 (Linear Regression Model), 以常態分佈殘差 (Residuals) 為基礎進行推估",
      "B": "使用平均數 (Mean) 與標準差 (Standard Deviation) 估計波動範圍",
      "C": "將資料裁剪至 ±3σ 範圍內以排除異常值影響",
      "D": "採用分位數回歸模型 (Quantile Regression Model), 聚焦於尾部分位 (Tail Quantiles) 以評估極端風險",
      "Ans": "D",
      "exp": "分位數迴歸不依賴常態分佈假設，並且允許分析師專注於分佈的特定分位點。透過聚焦尾部分位，可以精確地評估報酬率資料中極端損失事件（風險值）的影響，非常適合處理非對稱或有極端值的金融數據。"
    },
    {
      "id": "IPAS_02_L22_11411_34",
      "content": "在圖形資料庫 (Graph Database) 中建模社群平台資料時, 若每筆「按讚」行為都包含時間戳記 (Timestamp) 與裝置類型 (Device Type) 等資訊。若希望同時保留使用者與貼文之間的互動關係, 並能有效查詢「按讚」的行為屬性, 下列哪一種設計方式最為合適?",
      "A": "將「按讚」視為節點 (Node), 與使用者建立邊 (Edge)",
      "B": "將「按讚」資訊作為邊的屬性 (Property) 儲存, 連結使用者與被按讚的貼文節點",
      "C": "把「按讚」資訊直接寫入使用者節點中作為屬性",
      "D": "建立「按讚紀錄表」並將資料存入關聯式資料庫",
      "Ans": "B",
      "exp": "在圖形資料庫中，邊 (Edge) 代表實體（節點）之間的關係（例如「按讚」）。若關係本身帶有屬性（如時間、裝置類型），最標準且最有效率的設計是將這些資訊作為邊的屬性來儲存。"
    },
    {
      "id": "IPAS_02_L22_11411_35",
      "content": "某企業欲建構知識圖譜 (Knowledge Graph), 以整合內部的研究報告、專利資料與專家知識, 並支援語意查詢與關聯推理。若希望模型能具備良好的語意擴展性與高效推理能力, 下列哪一種圖模型設計最為合適?",
      "A": "僅以節點 (Node) 與邊 (Edge) 表示, 所有資訊存放於節點屬性中",
      "B": "將資料結構建為 RDF (Resource Description Framework) 三元組 (Subject - Predicate - Object)",
      "C": "使用文件型資料庫儲存內容, 並以標籤 (Tag) 連接節點",
      "D": "採用關聯式資料庫儲存對應關係, 並搭配預建索引加速查詢",
      "Ans": "B",
      "exp": "RDF 三元組 (Subject-Predicate-Object) 是構建知識圖譜最標準、最具有語意表達能力和擴展性的結構。它能清晰定義實體之間的關係類型（Predicate），從而有效支持複雜的語意查詢與關聯推理。"
    },
    {
      "id": "IPAS_02_L22_11411_36",
      "content": "某研究人員欲使用線性迴歸模型 (Linear Regression Model) 分析變數 Y 與 X 之間的關係, 但發現 Y 的分佈明顯右偏, 且其變異數隨 X 的增大而增加。為滿足模型假設並提升配適效果, 下列哪一種前處理方法最為合適?",
      "A": "對 X 進行標準化 (Standardization)",
      "B": "對 Y 進行 Box-Cox 轉換 (Box-Cox Transformation)",
      "C": "對資料進行一次差分 (First Differencing)",
      "D": "將 Y 中變異較大的樣本移除",
      "Ans": "B",
      "exp": "線性迴歸要求殘差常態分佈（對應 Y 右偏）和同方差性（對應變異數隨 X 增大而增加）。Box-Cox 轉換是常見的數據轉換技術，其主要目的就是對非常態分佈的變數進行轉換，使其更接近常態分佈，並有助於穩定變異數，從而滿足模型假設。"
    },
    {
      "id": "IPAS_02_L22_11411_37",
      "content": "若開發一個用於罕見疾病自動診斷的分類模型, 目前資料集中確診樣本僅佔不到 1%, 且因為標註成本高, 短期內無法取得更多資料。在此情況下, 若希望提升模型對少數類的偵測能力, 同時避免過擬合, 下列哪一種策略最為合理?",
      "A": "對少數類進行隨機過採樣 (Random Oversampling)",
      "B": "對多數類進行欠採樣 (Random Undersampling)",
      "C": "使用 SMOTE (Synthetic Minority Over-sampling Technique) 生成合成少數類樣本後再訓練分類模型",
      "D": "僅使用現有資料調整模型決策閾值 (Decision Threshold) 以提升召回率",
      "Ans": "C",
      "exp": "SMOTE 是一種改良的過採樣方法，它透過在現有少數類樣本之間合成新樣本來平衡類別，相比簡單的隨機過採樣，它更能有效避免模型過度學習重複樣本，從而降低過擬合風險。"
    },
    {
      "id": "IPAS_02_L22_11411_38",
      "content": "一家製造廠評估新生產線推出後, 產品良率是否較原生產線提升。工程師分別從兩條生產線各抽樣 100 件產品，原生產線良率為 95%, 新生產線為 97%。若欲檢定兩條生產線良率的差異是否具有統計意義，下列哪一種方法最為合適？",
      "A": "雙樣本平均數 t 檢定 (Two-sample t-test)",
      "B": "雙比例 Z 檢定 (Two-proportion Z-test)",
      "C": "卡方檢定 (Chi-square test)",
      "D": "變異數分析 (ANOVA)",
      "Ans": "B",
      "exp": "由於檢定對象是兩條生產線（雙樣本）的良率（以比例呈現），因此最合適的統計方法是雙比例 Z 檢定。"
    },
    {
      "id": "IPAS_02_L22_11411_39",
      "content": "若評估一個新開發的腫瘤分類模型，其資料集中有 80% 的樣本來自良性病例。若直接使用 5-fold 交叉驗證 (Cross-Validation) 進行模型評估，可能導致模型效能評估出現偏差，為避免此問題，下列哪一種作法最合適？",
      "A": "降低 K 值以減少交叉驗證次數",
      "B": "改為使用拔靴法 (Bootstrap)",
      "C": "調整測試集使良性樣本比例更高，以模擬真實分佈",
      "D": "使用分層交叉驗證 (Stratified K-Fold Cross-Validation), 以確保每折類別比例一致",
      "Ans": "D",
      "exp": "資料存在類別不平衡（80% 良性）。分層交叉驗證會強制確保每個訓練集和測試集（每一折）中的類別比例都與整個資料集的原始比例保持一致，從而得到更穩定、更公正的模型評估。"
    },
    {
      "id": "IPAS_02_L22_11411_40",
      "content": "請參考附圖 ，下列虛擬程式碼 (pseudocode) 最可能是在描述何種驗證法？",
      "A": "Hold-out 驗證 (Hold-out Validation)",
      "B": "留一交叉驗證 LOOCV (Leave-One-Out Cross Validation)",
      "C": "K-fold 交叉驗證 (K-fold Cross Validation)",
      "D": "拔靴法 (Bootstrap) 驗證",
      "Ans": "B",
      "exp": "虛擬程式碼顯示迴圈 N 次，每次都將一筆資料作為測試集，將其餘 N−1 筆作為訓練集。這就是 N 折交叉驗證，即留一交叉驗證 (LOOCV)。"
    },
    {
      "id": "IPAS_02_L22_11411_41",
      "content": "請參考附圖 ，下列虛擬程式碼 (pseudocode) 最可能是在描述何種演算法？",
      "A": "K-means 分群 (K-means Clustering)",
      "B": "高斯混合模型分群 (Gaussian Mixture Model Clustering)",
      "C": "階層式分群 (Hierarchical Clustering)",
      "D": "DBSCAN 分群 (Density-based Spatial Clustering of Applications with Noise Clustering)",
      "Ans": "A",
      "exp": "虛擬程式碼描述了：1. 隨機初始化 K 個中心點；2. 重複進行「將點分配給最近中心點」和「將中心點更新為群內平均值」的迭代過程。這正是K-means 分群演算法的核心步驟。"
    },
    {
      "id": "IPAS_02_L22_11411_42",
      "content": "考慮某生產線每小時出現瑕疵品的個數符合卜瓦松分佈 (Poisson Distribution)，已知平均每小時產生 5 個瑕疵品，附圖程式碼展示資料處理，請問下列敘述何者正確？",
      "A": "lambda_poisson = 5 表示每小時最多 5 個瑕疵品",
      "B": "poisson.pmf(5, lambda_poisson) 表示小於 5 個瑕疵品的機率",
      "C": "卜瓦松分佈的適用條件為事件彼此獨立，且平均發生率固定",
      "D": "poisson.cdf(10, 5) 表示大於或等於 10 個瑕疵品的機率",
      "Ans": "C",
      "exp": "卜瓦松分佈的適用條件是事件在固定區間內發生，且事件彼此獨立，同時平均發生率 (λ) 必須是固定的。"
    },
    {
      "id": "IPAS_02_L22_11411_43",
      "content": "分析師在載入資料後，檢視 Year 欄位的資料型態，發現它是 float64，而非一般年份常用的整數。他想了解這樣的情形為什麼會發生。請問下列哪些原因可能導致這種狀況？原因 A：CSV 檔中 Year 欄位有缺失值 (NaN)，導致 Pandas 自動將整欄轉為浮點數。原因 D：CSV 檔中的年份資料可能包含小數點 (例如 2006.0)，因此被視為浮點數。",
      "A": "原因 B、原因 C",
      "B": "原因 A、原因 D",
      "C": "原因 A、原因 B、原因 D",
      "D": "原因 C、原因 D",
      "Ans": "B",
      "exp": "由於整數型別 (如 int64) 在 pandas 中不支援缺失值 (NaN)，若欄位存在 NaN (原因 A)，pandas 會自動轉換為 float64。此外，若原始資料即為浮點數格式 (原因 D)，也會被讀取為 float64。"
    },
    {
      "id": "IPAS_02_L22_11411_44",
      "content": "研究團隊接下來想要將 Year 欄位轉換為整數型態，以便後續進行年份趨勢分析。考慮到資料中可能包含缺失值（NaN），請選出最合適的轉換方式。",
      "A": "data['Year'] = data['Year'].astype(int)",
      "B": "data['Year'] = data['Year'].fillna(0).astype(int)",
      "C": "data['Year'] = data['Year'].fillna(1).astype(int)",
      "D": "data['Year'] = data['Year'].astype('Int64')",
      "Ans": "D",
      "exp": "Int64 (注意大寫 I) 是 pandas 的可空整數型別 (Nullable Integer Type)，它允許整數欄位中包含缺失值 (NaN 或 pd.NA)，是處理帶有缺失值的整數資料最正確的方式。"
    },
    {
      "id": "IPAS_02_L22_11411_45",
      "content": "為了觀察各遊戲平台的市場表現，分析師想要統計每個平台的全球銷售總額，並以長條圖呈現。請選出最能正確實現此分析的程式碼。",
      "A": "data.groupby(\"Platform\")[\"Global_Sales\"].sum().plot(kind=\"bar\")",
      "B": "data.groupby(\"Platform\")[\"Global_Sales\"].count().plot(kind=\"bar\")",
      "C": "data[\"Platform\"].value_counts().plot(kind=\"bar\")",
      "D": "data.groupby(\"Platform\")[\"Global_Sales\"].mean().plot(kind=\"bar\")",
      "Ans": "A",
      "exp": "程式碼 (A) 透過 groupby(\"Platform\") 進行分組，並使用 .sum() 聚合計算出每個平台的銷售總額，完全符合要求。"
    },
    {
      "id": "IPAS_02_L22_11411_46",
      "content": "團隊希望比較北美、歐洲、日本及其他地區的整體銷售比例，並使用 seaborn 套件以長條圖的形式進行可視化分析。請選出能正確顯示這些地區銷售總額比例的程式碼。",
      "A": "sns.countplot(x=[\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\"], data=data)",
      "B": "sns.lineplot(x=\"Platform\", y=[\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\"], data=data)",
      "C": "sns.barplot(x=\"variable\", y=\"value\", data=pd.melt(data, value_vars=[\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\"]), estimator=sum)",
      "D": "sns.histplot(data[[\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\"]])",
      "Ans": "C",
      "exp": "由於四個地區的銷售額是四個不同的欄位，必須先使用 pd.melt() 將資料從寬格式轉換為包含地區名稱 (variable) 和銷售值 (value) 的長格式。然後使用 sns.barplot 並將 estimator 設為 sum 來計算和比較每個地區的總銷售額。"
    },
    {
      "id": "IPAS_02_L22_11411_47",
      "content": "研究團隊想要知道在北美地區 (NA) 銷售成績最好的遊戲前五名，並希望以 seaborn 的條狀圖呈現結果。請選出能正確完成這項分析的程式碼。",
      "A": "sns.barplot(x=\"NA_Sales\", y=\"Name\", data=data.head(5))",
      "B": "sns.barplot(x=\"Name\", y=\"NA_Sales\", data=data.nlargest(5, \"NA_Sales\"))",
      "C": "sns.lineplot(x=\"Name\", y=\"NA_Sales\", data=data.nlargest(5, \"NA_Sales\"))",
      "D": "sns.countplot(x=\"Name\", y=\"NA_Sales\", data=data)",
      "Ans": "B",
      "exp": "data.nlargest(5, \"NA_Sales\") 是正確的篩選方法，用於找出 NA_Sales 欄位數值最大的前 5 筆資料。接著使用 sns.barplot 將結果視覺化。"
    },
    {
      "id": "IPAS_02_L22_11411_48",
      "content": "根據上述結果 ，下列何者正確？",
      "A": "資料集個數為 199 筆，變數個數為 4 個",
      "B": "sales 變數的中位數是 16.827",
      "C": "facebook 變數的第三四分位數 (Q3) 是 11.94",
      "D": "youtube 變數的第一四分位數 (Q1) 是 89.25",
      "Ans": "D",
      "exp": "檢視 youtube 欄位下 25% (即第一四分位數 Q1) 的數值為 89.250000。"
    },
    {
      "id": "IPAS_02_L22_11411_49",
      "content": "參考下圖計算各變數的遺漏值 (NaN) 個數結果 ，下列何者正確？",
      "A": "選項 D",
      "B": "選項 B、選項 C、選項 D",
      "C": "選項 A、選項 C",
      "D": "選項 A、選項 B、選項 C",
      "Ans": "C",
      "exp": "在 pandas 中，檢查缺失值 (NaN) 的正確方法是 .isnull() 或 .isna()，它們的功能是等價的。搭配 .sum() 即可計算缺失值總數。"
    },
    {
      "id": "IPAS_02_L22_11411_50",
      "content": "考慮資料集已經填補遺漏值，參考下圖執行結果 ，下列何者正確？B：空格 1 完整語法 reg = LinearRegression().fit(X, y)；F：截距項係數值為 3.5561",
      "A": "B、C、F",
      "B": "B、F",
      "C": "A、C、D、F",
      "D": "B、E",
      "Ans": "B",
      "exp": "1. B 正確: sklearn.linear_model.LinearRegression().fit(X, y) 的輸入順序是 (自變數 X, 應變數 y)。2. F 正確: OLS 結果表中的 const (截距項) coef 數值為 3.5561。其他選項皆有誤，例如 C 的 reg.coef_ 不包含截距項， E 的 newspaper 變數在 α=0.05 下 $P>"
    }
  ]
}